version: "3.8"
services:
  controller:
    build:
      context: .
      args:
        - http_proxy=${http_proxy:-}
        - https_proxy=${https_proxy:-}
      tags:
        - ghcr.io/shoppal-ai/llava:dev
    env_file: .env
    ports:
      - "${CONTROLLER_HOST_PORT:-10000}:${CONTROLLER_PORT:-10000}"
    command:
      - python -m llava.serve.controller
      - --host 0.0.0.0
      - --port ${CONTROLLER_PORT:-10000}

  web_server:
    build:
      context: .
      args:
        - http_proxy=${http_proxy:-}
        - https_proxy=${https_proxy:-}
      tags:
        - ghcr.io/shoppal-ai/llava:dev
    env_file: .env
    ports:
      - "${WEB_HOST_PORT:-7860}:${WEB_PORT:-7860}"
    command:
      - python -m llava.serve.gradio_web_server
      - --controller http://controller:${CONTROLLER_PORT:-10000}
      - --model-list-mode reload
    depends_on:
      - controller

  model_worker:
    build:
      context: .
      args:
        - http_proxy=${http_proxy:-}
        - https_proxy=${https_proxy:-}
      tags:
        - ghcr.io/shoppal-ai/llava:dev
    env_file: .env
    ports:
      - "${MODEL_SERVER_HOST_PORT:-40000}:${MODEL_SERVER_PORT:-40000}"
    command:
      - python -m llava.serve.model_worker
      - --controller http://controller:${CONTROLLER_PORT:-10000}
      - --host 0.0.0.0
      - --port ${MODEL_SERVER_PORT:-40000}
      - --worker http://localhost:${MODEL_SERVER_PORT:-40000}
      - --model-path ${MODEL_PATH:-}
    volumes:
      - /data0:/data0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all # if you want to use all gpus, just set count: all
              # device_ids: ['4'] # if you want to use specific gpus, set device_ids: [1, 2, 3]
    depends_on:
      - controller

  cli:
    build:
      context: .
      args:
        - http_proxy=${http_proxy:-}
        - https_proxy=${https_proxy:-}
      tags:
        - ghcr.io/shoppal-ai/llava:dev
    env_file: .env
    ports:
      - "${MODEL_SERVER_HOST_PORT:-40000}:${MODEL_SERVER_PORT:-40000}"
    command:
      - python -m llava.serve.cli
      - --model-path ${MODEL_PATH:-}
      - --image-file "https://llava-vl.github.io/static/images/view.jpg"
      - --load-4bit

  train:
    build:
      context: .
      args:
        - http_proxy=${http_proxy:-}
        - https_proxy=${https_proxy:-}
      tags:
        - ghcr.io/shoppal-ai/llava:dev
    shm_size: "100gb"
    env_file: .env
    command:
      - deepspeed
      - /workspace/llava/train/train_mem.py
      - --deepspeed=./scripts/zero3.json 
      - --model_name_or_path=lmsys/vicuna-13b-v1.5 
      - --version=v1 
      - --data_path=${TRAIN_DATA_JSON} 
      - --image_folder=${TRAIN_IMAGE_ROOT}
      - --vision_tower=openai/clip-vit-large-patch14-336 
      - --pretrain_mm_mlp_adapter=${PRETRAIN_MM_BIN_FILE}
      - --mm_projector_type=mlp2x_gelu 
      - --mm_vision_select_layer=-2 
      - --mm_use_im_start_end=False 
      - --mm_use_im_patch_token=False 
      - --image_aspect_ratio=pad 
      - --group_by_modality_length=True 
      - --bf16=True 
      - --output_dir=${OUTPUT_DIR:-llava-v1.5-13b}
      - --num_train_epochs=1 
      - --per_device_train_batch_size=16 
      - --per_device_eval_batch_size=4 
      - --gradient_accumulation_steps=1 
      - --evaluation_strategy=no 
      - --save_strategy=steps 
      - --save_steps=50000 
      - --save_total_limit=1 
      - --learning_rate=2e-5 
      - --weight_decay=0. 
      - --warmup_ratio=0.03 
      - --lr_scheduler_type=cosine 
      - --logging_steps=1 
      - --tf32=True 
      - --model_max_length=2048 
      - --gradient_checkpointing=True 
      - --dataloader_num_workers=4 
      - --lazy_preprocess=True 
      - --report_to=wandb
    volumes:
      - /data0:/data0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all # if you want to use all gpus, just set count: all
              # device_ids: ['4'] # if you want to use specific gpus, set device_ids: [1, 2, 3]
